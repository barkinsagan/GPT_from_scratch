{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory stats 1:\n",
      "Allocated: 0.02 GB\n",
      "Cached: 0.07 GB\n",
      "Output text:\n",
      " Every effort moves you Spirits caves Feng lith cohorts Physical Position Wind Bars Vern\n",
      "Memory stats 2:\n",
      "Allocated: 0.02 GB\n",
      "Cached: 0.07 GB\n",
      "Memory stats 3:\n",
      "Allocated: 0.67 GB\n",
      "Cached: 0.72 GB\n",
      "Memory stats 4:\n",
      "Allocated: 0.67 GB\n",
      "Cached: 0.72 GB\n",
      "Memory stats 5:\n",
      "Allocated: 0.67 GB\n",
      "Cached: 0.72 GB\n",
      "Ep 1 (Step 000000): Train loss 9.913, Val loss 10.196\n",
      "Ep 1 (Step 000005): Train loss 8.295, Val loss 8.533\n",
      "Ep 1 (Step 000010): Train loss 7.293, Val loss 7.420\n",
      "Ep 1 (Step 000015): Train loss 6.455, Val loss 6.818\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 2 (Step 000020): Train loss 6.197, Val loss 6.638\n",
      "Ep 2 (Step 000025): Train loss 5.671, Val loss 6.500\n",
      "Ep 2 (Step 000030): Train loss 5.826, Val loss 6.498\n",
      "Ep 2 (Step 000035): Train loss 5.530, Val loss 6.482\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      3\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfinal_gpt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPTModel\n\u001b[1;32m      8\u001b[0m GPT_CONFIG_124M \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m50257\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m,    \u001b[38;5;66;03m#1\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqkv_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     16\u001b[0m }\n\u001b[1;32m     17\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m123\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/NLP/final_gpt.py:402\u001b[0m\n\u001b[1;32m    397\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(\n\u001b[1;32m    398\u001b[0m      model\u001b[38;5;241m.\u001b[39mparameters(),           \u001b[38;5;66;03m#1\u001b[39;00m\n\u001b[1;32m    399\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0004\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m    400\u001b[0m )\n\u001b[1;32m    401\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m--> 402\u001b[0m train_losses, val_losses, tokens_seen \u001b[38;5;241m=\u001b[39m train_model_simple(\n\u001b[1;32m    403\u001b[0m     model, train_loader, val_loader, optimizer, device,\n\u001b[1;32m    404\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs, eval_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, eval_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m    405\u001b[0m     start_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvery effort moves you\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[1;32m    406\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/NLP/final_gpt.py:384\u001b[0m, in \u001b[0;36mtrain_model_simple\u001b[0;34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer)\u001b[0m\n\u001b[1;32m    378\u001b[0m             track_tokens_seen\u001b[38;5;241m.\u001b[39mappend(tokens_seen)\n\u001b[1;32m    379\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEp \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglobal_step\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m06d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    380\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    381\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m             )\n\u001b[0;32m--> 384\u001b[0m     generate_and_print_sample(                      \u001b[38;5;66;03m#7\u001b[39;00m\n\u001b[1;32m    385\u001b[0m         model, tokenizer, device, start_context\n\u001b[1;32m    386\u001b[0m     )\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_losses, val_losses, track_tokens_seen\n",
      "File \u001b[0;32m~/Projects/NLP/final_gpt.py:346\u001b[0m, in \u001b[0;36mgenerate_and_print_sample\u001b[0;34m(model, tokenizer, device, start_context)\u001b[0m\n\u001b[1;32m    344\u001b[0m encoded \u001b[38;5;241m=\u001b[39m text_to_token_ids(start_context, tokenizer)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 346\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m generate_text_simple(\n\u001b[1;32m    347\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel, idx\u001b[38;5;241m=\u001b[39mencoded,\n\u001b[1;32m    348\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, context_size\u001b[38;5;241m=\u001b[39mcontext_size\n\u001b[1;32m    349\u001b[0m     )\n\u001b[1;32m    350\u001b[0m decoded_text \u001b[38;5;241m=\u001b[39m token_ids_to_text(token_ids, tokenizer)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28mprint\u001b[39m(decoded_text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m))      \u001b[38;5;66;03m#1\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/NLP/final_gpt.py:70\u001b[0m, in \u001b[0;36mgenerate_text_simple\u001b[0;34m(model, idx, max_new_tokens, context_size)\u001b[0m\n\u001b[1;32m     68\u001b[0m idx_cond \u001b[38;5;241m=\u001b[39m idx[:, \u001b[38;5;241m-\u001b[39mcontext_size:]    \u001b[38;5;66;03m#2\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 70\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(idx_cond)\n\u001b[1;32m     72\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]                    \u001b[38;5;66;03m#3\u001b[39;00m\n\u001b[1;32m     73\u001b[0m probas \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)           \u001b[38;5;66;03m#4\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/NLP/final_gpt.py:151\u001b[0m, in \u001b[0;36mGPTModel.forward\u001b[0;34m(self, in_idx)\u001b[0m\n\u001b[1;32m    149\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_embeds \u001b[38;5;241m+\u001b[39m pos_embeds\n\u001b[1;32m    150\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_emb(x)\n\u001b[0;32m--> 151\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrf_blocks(x)\n\u001b[1;32m    152\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_norm(x)\n\u001b[1;32m    153\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_head(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/NLP/final_gpt.py:100\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m shortcut \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     99\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[0;32m--> 100\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt(x)\n\u001b[1;32m    101\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_shortcut(x)\n\u001b[1;32m    102\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m shortcut      \u001b[38;5;66;03m#2\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/NLP/final_gpt.py:28\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     27\u001b[0m     b, num_tokens, d_in \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 28\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_key(x)         \u001b[38;5;66;03m#3\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_query(x)    \u001b[38;5;66;03m#3\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_value(x)     \u001b[38;5;66;03m#3\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "from final_gpt import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 4,    #1\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12, \n",
    "    \"drop_rate\": 0.1,       #2\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions to convert text to tokens and back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you system constructor throwbanrestrictedbow dossierstruction Copenhagen URL\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "from final_gpt import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)    #1\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)                #2\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate model performance, we need to compute the perplexity of the model on a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107, 588, 11311]])  #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():     #1\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)     #2\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[45909],\n",
      "         [44465],\n",
      "         [ 8693]],\n",
      "\n",
      "        [[45076],\n",
      "         [24268],\n",
      "         [37081]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  HIM535 Bowl\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\"\n",
    "      f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see initial softmax probability scores for the target tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([1.8156e-05, 2.0161e-05, 1.6834e-05])\n",
      "Text 2: tensor([1.2397e-05, 2.2042e-05, 1.1297e-05])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to maximize the probability of the correct target token and punish the model for predicting the wrong token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.9165, -10.8118, -10.9921, -11.2980, -10.7225, -11.3910])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-11.0220)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.0220)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we want to minimize in other words cross-entropy loss.\n",
    "\n",
    "Cross-entropy loss measures the distance between the predicted probability distribution and the true probability distribution.\n",
    "\n",
    "Loss becomes zero when the predicted probability distribution is exactly the same as the true probability distribution.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flattening it is the same as combining batch and sequence dimensions together like  they were one long sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.0220)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)    #1\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride): \n",
    "            # shifting the y by 1 to get desired tokens for each step.\n",
    "            # max length is the context size.\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):    #3\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):         #4\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "        tokenizer = tiktoken.get_encoding(\"gpt2\")                         #1\n",
    "        dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)   #2\n",
    "        dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,     #3\n",
    "        num_workers=num_workers     #4\n",
    "    )\n",
    "        return dataloader        \n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([2, 4]) torch.Size([2, 4])\n",
      "torch.Size([1, 4]) torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)         #1\n",
    "    target_batch = target_batch.to(device)      \n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)     #1\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))   #2\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()    #3\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches    #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.992170822289255\n",
      "Validation loss: 10.986001213984704\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)   #1\n",
    "with torch.no_grad():                                        #2\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)    #3\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm is as follows:\n",
    "\n",
    "1.Iterate over the training epochs.\n",
    "2.Iteraite over the training batches in that epoch.\n",
    "3.Reset the gradients from the previous batch.\n",
    "4.Calculate the loss for the batch.\n",
    "5.Backpropagate the loss.\n",
    "6.Update the model parameters.\n",
    "7.Calculate the validation loss.\n",
    "8.Print the training and validation losses.\n",
    "9.Save the model if the validation loss is the lowest.\n",
    "10.Generate sample text from the model.\n",
    "11.Go back to step 1 if there are more epochs to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()  #1\n",
    "    with torch.no_grad():                              #2\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))      #1\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader,\n",
    "                       optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []    #1\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):    #2\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()   #3\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward()                     #4\n",
    "            optimizer.step()                    #5\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:    #6\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, \"\n",
    "                      f\"Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "        generate_and_print_sample(                      #7\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "     model.parameters(),           #1\n",
    "    lr=0.0004, weight_decay=0.1\n",
    ")\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(\n",
    "        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()                   #1\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)     #2\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of always picking the most likely next token, we can sample from the model's predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123) \n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x moves\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item()\n",
    "             for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "temperatures = [1, 0.1, 5]                                     #1\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T)\n",
    "                for T in temperatures]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], \n",
    "                   bar_width, label=f'Temperature = {T}')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we had logits [0.1, 0.2, 0.3, 0.4] and we wanted to scale them by a temperature of 0.1.\n",
    "\n",
    "Then we would have the following scaled logits:\n",
    "\n",
    "tensor([0.0321, 0.0871, 0.2369, 0.6439])\n",
    "\n",
    "\n",
    "We can see that if temperature is 0.1, the model is more confident in its predictions and makes more sharp predictions getting close to the argmax approach\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0321, 0.0871, 0.2369, 0.6439])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_with_temperature(torch.tensor([0.1, 0.2, 0.3, 0.4] ), 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result may also result in gramaticaly incorrect predictions.\n",
    "\n",
    "So we can improve the results by using top-k sampling.\n",
    "\n",
    "While sampling we can only sample from the top k tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],    #1\n",
    "    input=torch.tensor(float('-inf')),     #2\n",
    "    other=next_token_logits     #3\n",
    ")\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();\n",
    "\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the weights of GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x78dbedb0e000>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    \"LLMs-from-scratch/main/ch05/\"\n",
    "    \"01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 13:55:18.023637: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-12 13:55:18.033261: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739357718.045804    6983 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739357718.049562    6983 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-12 13:55:18.062101: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 93.0kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:12<00:00, 82.0kiB/s]\n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 107kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [15:00<00:00, 553kiB/s]    \n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 4.92MiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:01<00:00, 434kiB/s]  \n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 514kiB/s]  \n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=\"124M\", models_dir=\"gpt2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory stats 1:\n",
      "Allocated: 1.11 GB\n",
      "Cached: 1.38 GB\n",
      "Output text:\n",
      " Every effort moves you Spirits caves Feng lith cohorts Physical Position Wind Bars Vern\n",
      "Memory stats 2:\n",
      "Allocated: 1.11 GB\n",
      "Cached: 1.38 GB\n",
      "Memory stats 3:\n",
      "Allocated: 1.77 GB\n",
      "Cached: 1.88 GB\n",
      "Memory stats 4:\n",
      "Allocated: 1.77 GB\n",
      "Cached: 1.88 GB\n",
      "Memory stats 5:\n",
      "Allocated: 1.77 GB\n",
      "Cached: 1.88 GB\n"
     ]
    }
   ],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 4,    #1\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12, \n",
    "    \"drop_rate\": 0.1,       #2\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024})\n",
    "NEW_CONFIG.update({\"qkv_bias\": True})\n",
    "\n",
    "\n",
    "from final_gpt import GPTModel\n",
    "\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()\n",
    "\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \"\n",
    "                          \"Right: {right.shape}\"\n",
    "        )\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):           #1\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):     #2\n",
    "        q_w, k_w, v_w = np.split(                            #3\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])    #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
